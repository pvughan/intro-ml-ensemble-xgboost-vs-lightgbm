{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e6f8bb8",
   "metadata": {},
   "source": [
    "# XGBoost vs LightGBM â€“ Interactive Comparative Study\n",
    "\n",
    "This notebook provides an **interactive, step-by-step exploration** of the benchmark comparison between XGBoost and LightGBM.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. ðŸ“Š Load and explore dataset\n",
    "2. ðŸ¤– Train both models with baseline configurations\n",
    "3. ðŸ“ˆ Evaluate using **5 performance benchmarks**\n",
    "4. ðŸ”¬ Run systematic hyperparameter ablation study\n",
    "5. ðŸ“‰ Visualize comprehensive benchmark results\n",
    "\n",
    "**Note:** This notebook uses the same `src/` modules as `main.py` to ensure consistency and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## Setup: Import Modules\n",
    "\n",
    "We import from the project's `src/` directory to use the official pipeline architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m sys.path.insert(\u001b[32m0\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msrc\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Import project modules\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_data\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mxgboost_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_xgboost_model\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlightgbm_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lightgbm_model\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ml\\intro-ml-ensemble-xgboost-vs-lightgbm\\src\\data\\loader.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_breast_cancer\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_data\u001b[39m(test_size=\u001b[32m0.2\u001b[39m, random_state=\u001b[32m42\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "# Import project modules\n",
    "from data.loader import load_data\n",
    "from models.xgboost_model import get_xgboost_model\n",
    "from models.lightgbm_model import get_lightgbm_model\n",
    "from evaluation.evaluator import evaluate_model\n",
    "from evaluation.ablation import run_ablation\n",
    "from utils.metrics import print_metrics\n",
    "\n",
    "# Visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-section",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "\n",
    "Using the **Breast Cancer dataset** from scikit-learn:\n",
    "- Binary classification task\n",
    "- 30 features (tumor characteristics)\n",
    "- 80/20 train/test split with fixed random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "print(f\"\\nâœ… Data loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baseline-section",
   "metadata": {},
   "source": [
    "## 2. Train Baseline Models\n",
    "\n",
    "Training both gradient boosting frameworks with default hyperparameters:\n",
    "- **XGBoost**: Traditional gradient boosting with level-wise tree growth\n",
    "- **LightGBM**: Gradient boosting with leaf-wise tree growth\n",
    "\n",
    "### XGBoost Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-xgb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train XGBoost model\n",
    "xgb_model = get_xgboost_model()\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "print(\"âœ… XGBoost training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lgbm-train",
   "metadata": {},
   "source": [
    "### LightGBM Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-lgbm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train LightGBM model\n",
    "lgbm_model = get_lightgbm_model()\n",
    "print(\"Training LightGBM...\")\n",
    "lgbm_model.fit(X_train, y_train)\n",
    "print(\"âœ… LightGBM training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-section",
   "metadata": {},
   "source": [
    "## 3. Evaluate Using 5 Performance Benchmarks\n",
    "\n",
    "Each model is evaluated using **5 different performance metrics** for comprehensive comparison:\n",
    "\n",
    "1. **Accuracy** - Overall classification correctness\n",
    "2. **Precision** - Positive prediction accuracy (minimize false positives)\n",
    "3. **Recall** - True positive detection rate (minimize false negatives)\n",
    "4. **F1-Score** - Harmonic mean balancing precision and recall\n",
    "5. **ROC-AUC** - Area under ROC curve (probability ranking quality)\n",
    "\n",
    "### XGBoost Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-xgb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_results = evaluate_model(xgb_model, X_test, y_test)\n",
    "print_metrics(\"XGBoost Baseline\", xgb_results)\n",
    "\n",
    "# Store for visualization\n",
    "xgb_baseline_df = pd.DataFrame([xgb_results])\n",
    "xgb_baseline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-lgbm",
   "metadata": {},
   "source": [
    "### LightGBM Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-lgbm-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_results = evaluate_model(lgbm_model, X_test, y_test)\n",
    "print_metrics(\"LightGBM Baseline\", lgbm_results)\n",
    "\n",
    "# Store for visualization\n",
    "lgbm_baseline_df = pd.DataFrame([lgbm_results])\n",
    "lgbm_baseline_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-baseline",
   "metadata": {},
   "source": [
    "### Baseline Comparison Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "xgb_scores = [xgb_results[m] for m in metrics]\n",
    "lgbm_scores = [lgbm_results[m] for m in metrics]\n",
    "\n",
    "# Create comparison plot\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "x = range(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar([i - width/2 for i in x], xgb_scores, width, \n",
    "               label='XGBoost', color='#FF6B6B', alpha=0.8)\n",
    "bars2 = ax.bar([i + width/2 for i in x], lgbm_scores, width,\n",
    "               label='LightGBM', color='#4ECDC4', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Baseline Performance Comparison - 5 Benchmarks', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([m.upper().replace('_', '-') for m in metrics])\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim(0.9, 1.0)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "               f'{height:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Baseline comparison visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ablation-section",
   "metadata": {},
   "source": [
    "## 4. Systematic Hyperparameter Ablation Study\n",
    "\n",
    "Now we **benchmark both methods on different settings of hyperparameters** to analyze:\n",
    "- Model sensitivity to hyperparameter changes\n",
    "- Optimal configuration discovery\n",
    "- Performance stability across settings\n",
    "\n",
    "### Hyperparameter Search Space:\n",
    "- **Learning Rate**: [0.01, 0.1]\n",
    "- **N Estimators**: [100, 300]\n",
    "\n",
    "**Total Experiments**: 2 models Ã— 4 configurations Ã— 5 metrics = **40 benchmark measurements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ablation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ”¬ Running comprehensive ablation study...\")\n",
    "print(\"This will train and evaluate 8 model configurations.\\n\")\n",
    "\n",
    "ablation_df = run_ablation(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(\"\\nâœ… Ablation study complete!\")\n",
    "print(f\"\\nResults saved to: src/ablation_results.csv\")\n",
    "print(f\"\\nTotal configurations tested: {len(ablation_df)}\")\n",
    "\n",
    "# Display results\n",
    "ablation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ablation-viz",
   "metadata": {},
   "source": [
    "### Ablation Results Visualization\n",
    "\n",
    "Visualizing how different hyperparameter settings affect performance across all 5 benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-ablation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive ablation visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics_cols = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']\n",
    "\n",
    "for idx, metric in enumerate(metrics_cols):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create grouped data\n",
    "    df_pivot = ablation_df.pivot_table(\n",
    "        values=metric,\n",
    "        index=['learning_rate', 'n_estimators'],\n",
    "        columns='model'\n",
    "    )\n",
    "    \n",
    "    df_pivot.plot(kind='bar', ax=ax, width=0.8, color=['#FF6B6B', '#4ECDC4'])\n",
    "    ax.set_title(f'{metric.upper().replace(\"_\", \"-\")} Across Configurations', \n",
    "                 fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel('(Learning Rate, N Estimators)', fontsize=9)\n",
    "    ax.set_ylabel(metric.capitalize(), fontsize=9)\n",
    "    ax.legend(title='Model', loc='lower right', fontsize=9)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Format x-axis labels\n",
    "    labels = [f\"({lr}, {n})\" for lr, n in df_pivot.index]\n",
    "    ax.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)\n",
    "\n",
    "# Remove extra subplot\n",
    "fig.delaxes(axes[5])\n",
    "\n",
    "plt.suptitle('Ablation Study: All 5 Benchmarks Across Hyperparameter Configurations', \n",
    "             fontsize=14, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Ablation visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "best-config",
   "metadata": {},
   "source": [
    "### Find Best Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "find-best",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best configuration for each model based on accuracy\n",
    "xgb_best = ablation_df[ablation_df['model'] == 'XGBoost'].loc[\n",
    "    ablation_df[ablation_df['model'] == 'XGBoost']['accuracy'].idxmax()\n",
    "]\n",
    "\n",
    "lgbm_best = ablation_df[ablation_df['model'] == 'LightGBM'].loc[\n",
    "    ablation_df[ablation_df['model'] == 'LightGBM']['accuracy'].idxmax()\n",
    "]\n",
    "\n",
    "print(\"ðŸ† Best Configurations (by Accuracy):\\n\")\n",
    "print(f\"XGBoost:  lr={xgb_best['learning_rate']}, n_estimators={int(xgb_best['n_estimators'])}\")\n",
    "print(f\"  â†’ Accuracy: {xgb_best['accuracy']:.4f}, F1: {xgb_best['f1']:.4f}, ROC-AUC: {xgb_best['roc_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nLightGBM: lr={lgbm_best['learning_rate']}, n_estimators={int(lgbm_best['n_estimators'])}\")\n",
    "print(f\"  â†’ Accuracy: {lgbm_best['accuracy']:.4f}, F1: {lgbm_best['f1']:.4f}, ROC-AUC: {lgbm_best['roc_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## 5. Summary & Key Insights\n",
    "\n",
    "### What We Did:\n",
    "âœ… Loaded Breast Cancer dataset (569 samples, 30 features)  \n",
    "âœ… Trained XGBoost and LightGBM with baseline configurations  \n",
    "âœ… Evaluated using **5 performance benchmarks** (Accuracy, Precision, Recall, F1, ROC-AUC)  \n",
    "âœ… Conducted systematic hyperparameter ablation (**8 total configurations**)  \n",
    "âœ… Visualized comprehensive benchmark results  \n",
    "\n",
    "### Key Findings:\n",
    "- Both models achieve excellent performance (>96% accuracy)\n",
    "- Performance is relatively stable across hyperparameter settings\n",
    "- ROC-AUC scores are consistently high (>0.98) for both models\n",
    "- Detailed ablation results saved to `src/ablation_results.csv`\n",
    "\n",
    "### Next Steps:\n",
    "For production-ready visualizations, run:\n",
    "```bash\n",
    "python src/evaluation/visualize.py\n",
    "```\n",
    "\n",
    "This generates 4 publication-quality charts in the `plots/` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
